{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess9_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: Clustering and PCA Analysis\n",
        "Dani Bauer, 2022\n",
        "\n",
        "As mentioned in the beginning of lecture, there are (at least) two basic learning setups.  In **supervised** machine learning one observes a response $Y$ with observing $p$ different features $X=(X_1,X_2,\\ldots,X_p)$, where we typically postulate the relationship $Y = f(X)+\\varepsilon$ and $\\varepsilon$ independent of $X$ with mean zero. Here quality is usually assessed by the (test/out-of-sample) error that compares predictions and realizations for a separate dataset.  In **unsupervised** learning, we only observe $p$ features $X_1,X_2,\\ldots,X_p$, and we would like to learn about their relationship -- without focussing on a supervising outcome.  Of course, the difficulty is how to assess quality in this case -- so different unsupervised learning techniques are very different, and which one to pick will depend on the nature of the problem.\n",
        "\n",
        "In this tutorial, we will take a closer look at two algorithms: **Principal Component Analysis (PCA)** and **Clustering**.  There are variety of other techniques, including anomaly detection, self-organizing maps, association analysis, etc.\n",
        "\n",
        "As usual, we start by implementing the relevant packages:"
      ],
      "metadata": {
        "id": "aIiejbOjljzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import matplotlib.lines as mlines\n",
        "import pandas as pd   \n",
        "import seaborn as sns\n",
        "from random import sample\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler # For rescaling metrics to fit 0 to 1 range\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import euclidean"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis\n",
        "\n",
        "## Background\n",
        "\n",
        "The key idea behind *Principal Component Analysis* (PCA) is to use \"meaningful\" linear combinations of the data:\n",
        "$$\n",
        "Z_m = \\sum_{j=1}^p \\phi_{jm} X_j \\text{ such that }\\sum_{j=1}^p \\phi^2_{jm} = 1,\n",
        "$$\n",
        "where the idea is to choose:\n",
        "\n",
        "- $\\phi_{j1},\\,j=1,\\ldots,p$  such that the the variance of $Z_1$ is maximal (i.e., so that it captures most of the variation in the $X$s)\n",
        "\n",
        "- $\\phi_{j2},\\,j=1,\\ldots,p$ such that the variance of $Z_2$ is maximial out of all the variables that are uncorrelated with $Z_1$.\n",
        "\n",
        "- $\\phi_{j3},\\,j=1,\\ldots,p$ such that the variance of $Z_3$ is maximial out of all the variables that are uncorrelated with $Z_1$ and $Z_2$. $\\ldots$\n",
        "\n",
        "Hence, one way to intepret the principal components are the linear combination that best reflect the variation in the data.\n",
        "\n",
        "Importantly, the scale of variables matters, so it is a good idea to center and scale your variables.  Also, the components are only determined up to their sign (plus/minus).  To discern how important different variables are, one typically considers the *Percent of Variance Explained* (PVE):\n",
        "$$\n",
        "\\text{PVE}_m = \\frac{\\sum_{i=1}^n \\left(\\sum_{j=1}^p \\phi_{jm}x_{ij}\\right)^2}{\\sum_{i=1}^n \\sum_{j=1}^p x_{ij}^2},\n",
        "$$\n",
        "and the resulting plot that depicts PVE by principal components is referred to as a *scee plot*.\n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a basic example, where we simulate heights and weights of a fictional population according to some arbitrary parameters.  More precisely, we assume that weight and height follow Normal distributions with a mean weight (in kg) of 70 and a mean height (in cm) of 170m, and variances of 25 and 150, respectively.  We assume the correlation parameter is 50%.\n"
      ],
      "metadata": {
        "id": "M0g7gj9NmeKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = (70,170)\n",
        "cov = np.array([[25, np.sqrt(25)*np.sqrt(150)*0.5], [np.sqrt(25)*np.sqrt(150)*0.5, 150]])\n",
        "X_raw = np.random.multivariate_normal(mu, cov, size=5000)\n"
      ],
      "metadata": {
        "id": "WUPuLhokm73m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check quick:"
      ],
      "metadata": {
        "id": "nwTiQwq_p0Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw.mean(axis=0)"
      ],
      "metadata": {
        "id": "bl3Q6vB_prhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.cov(np.transpose(X_raw))"
      ],
      "metadata": {
        "id": "NmFiKEcHqCjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the *empirical* moments look similar to our theoretical moments. Let's take a peak:\n"
      ],
      "metadata": {
        "id": "8qFiIVu4p161"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[:,0], X_raw[:,1], s = 1, color = 'black')\n",
        "plt.xlabel('weight')\n",
        "plt.ylabel('height')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tY1m2LNRpBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prinicipal components are closely related to the eigen-analysis (eigenvalues and eigenvectors) of the correlation (or covariance) matrix.  Let's illustrate (if you never had a linear algebra class, feel free to skip this part).  First, let's scale the data and calculate the (empirical) correlation matrix -- remember, in practical settings we usually don't know the underlying parameters:\n"
      ],
      "metadata": {
        "id": "_IXLO-52q0zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(np.transpose(X_raw))\n",
        "R"
      ],
      "metadata": {
        "id": "5VahnsYuq1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the eigenvalues and the eigenvectors of `R`.  As a reminder, the eigenvectors decompose a matrix (a.k.a. a linear mapping in finite dimensions) in orthogonal directions, whereas the eignvalues provide the \"length\" (importance) of the directions.  In particular, we can represent a symmetric matrix in diagnolized form by relying on eigenvectors and eigenvalues. "
      ],
      "metadata": {
        "id": "mdNuryMYrJt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dec = np.linalg.eig(R)\n",
        "Dec"
      ],
      "metadata": {
        "id": "rpMC6B0drS6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = Dec[1] # Matrix of Eigenvectores\n",
        "D = np.diag(Dec[0]) # Diagonal Matrix of Eigenvalues\n",
        "np.dot(np.dot(V,D),np.transpose(V)) # Calculates V * D * V'"
      ],
      "metadata": {
        "id": "oGS9R-CxsVgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get intuition, let's plot the Eigenvectors:"
      ],
      "metadata": {
        "id": "yUlyHgUy3IyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_raw)\n",
        "plt.figure(figsize = (6,4))\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(scaler.transform(X_raw)[:,0], scaler.transform(X_raw)[:,1], s = 1, color = 'black')\n",
        "line1 = mlines.Line2D([0, 0.7071], [0, -.7071], color='red')\n",
        "line2 = mlines.Line2D([0, .7071], [0, .7071], color='yellow')\n",
        "transform = ax.transAxes\n",
        "line.set_transform(transform)\n",
        "ax.add_line(line1)\n",
        "ax.add_line(line2)\n",
        "plt.xlabel('scaled weight')\n",
        "plt.ylabel('scaled height')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_qNILkH44nBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is exactly this notion of \"importance\" that motivates principal components.  Indeed, the loadings of the principal components just amount to the *ordered* eigenvalues..."
      ],
      "metadata": {
        "id": "D100cSoIsTRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "## Background\n",
        "\n",
        "*Clustering* refers to techniques for finding subgroups in a given dataset. The typical approach to determine clusters $C_1,\\ldots,C_K$ is to minimize:\n",
        "$$\n",
        "\\sum_{k=1}^K W(C_k),\n",
        "$$\n",
        "where $W$ is a measure of *variation* within a cluster.  For instance, **k-means clustering** uses the Euclidean distance to measure variation:\n",
        "$$\n",
        "W(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2.\n",
        "$$\n",
        "The algorithms are implemented via a greedy algorithm by considering the centers of clusters (referred to as *centroids*).  The number of clusters $K$ must be chosen beforehand.  One approach is *hierarchical clustering*, where one starts with a larger number of clusters and then *fuses* custers that are similar (e.g., with regards to the distance between their centroids). \n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a very basic simulated example -- let's simulate normal random variables with different means:"
      ],
      "metadata": {
        "id": "OEru_Pl4ACGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw2 = np.random.multivariate_normal((0,0), np.array([[1, 0], [0, 1]]), size=100)\n",
        "X_raw2[0:49,0]=X_raw2[1:50,0]+3\n",
        "X_raw2[0:49,1]=X_raw2[1:50,1]-4"
      ],
      "metadata": {
        "id": "tCFUcnLqbdpU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot:"
      ],
      "metadata": {
        "id": "Ozl7WSIvbdMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw2[0:49,0], X_raw2[0:49,1], color='red')\n",
        "plt.scatter(X_raw2[50:99,0], X_raw2[50:99,1], color='black')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AG2cFW5VctpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's run k means clustering:\n"
      ],
      "metadata": {
        "id": "5WdTAU69d6rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "kmeans.fit(X_raw2)\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroids"
      ],
      "metadata": {
        "id": "oq_YzcFwd6dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = kmeans.fit_predict(X_raw2)\n",
        "label"
      ],
      "metadata": {
        "id": "PhY9123eecf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the algorithm was able to identify how we set up the data!"
      ],
      "metadata": {
        "id": "TPcmD4aOeq3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: County Health Rankings 2013\n",
        "\n",
        "We analyze [County Health Rankings](www.countyhealthrankings.org) in the US in 2013, based on a data set from the University of Wisconsin Population Health Institute.\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "3RW20yqheke7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge_n_HRNgRm0"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whMtWdEgtAh"
      },
      "source": [
        "health = pd.read_csv('CAS_PredMod/pa_data_countyHealthRR.csv')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "V3w077i9fShr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, we have a bunch of missing data. Let's drop the columns where we have lots of missing values and then drop na-s:"
      ],
      "metadata": {
        "id": "4bfsz1CRe7y9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012bX41Lg9dF"
      },
      "source": [
        "health = health.drop(columns=['FIPS','State','County','Perc.Fair.Poor.Health','Perc.Smokers','Perc.Excessive.Drinking','MV.Mortality.Rate','Pr.Care.Physician.Ratio','Perc.No.Soc.Emo.Support'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm0Hb3CxhytE"
      },
      "source": [
        "health = health.dropna()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVSWGGQ-kVhg"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(health)\n",
        "health_sc = scaler.transform(health)\n",
        "health_sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzeYPjzOmYs5"
      },
      "source": [
        "K-Means clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF4Tvu-imXH7"
      },
      "source": [
        "wcss = []\n",
        "for i in range(2, 12):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "    kmeans.fit(health_sc)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.plot(range(2, 12), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1JGhuVnqGv"
      },
      "source": [
        "kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "kmeans.fit(health_sc)\n",
        "kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4l2tN9WpQmC"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMy-DCfwo9SH"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(health_sc)\n",
        "principalComponents"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}