{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess9_WineExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analyzing Wine Data via Cluster and PCA"
      ],
      "metadata": {
        "id": "54zdirPNoTex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we return to the wine dataset, we are not concerned about understanding the types of wines there are out there. Before, we tried to predict the quality of the wine based on the other features. This time, we are trying to understand the relationships between each of the variables. To tackle this issue, we will use k-means clustering to group the wines and use principal component analysis to reduce a highly-dimensional dataset."
      ],
      "metadata": {
        "id": "WC9dzqQxoZ36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation and Cleaning"
      ],
      "metadata": {
        "id": "4KMRO0cjowEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "V5AV3m4CPRFM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcrEJmCwPKYG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('CAS_PredMod/pa_data_winequality-red.csv', sep = ';')"
      ],
      "metadata": {
        "id": "_elvDLM2PQVh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.describe"
      ],
      "metadata": {
        "id": "olXoLsnePWcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine.info()"
      ],
      "metadata": {
        "id": "hifwUuIUPXvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For clustering, we need to scale each variable (as long as it's continuous data). That is, we need to make sure each variable is relatable to one another. We cannot compare 1 meter to 1 mile. That concept applies to this dataset. Since clustering focuses on minimizing the distance between data points, we cannot have them on separate scales such as pH and wine quality. "
      ],
      "metadata": {
        "id": "lRZx6Vi-o7in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(wine)\n",
        "wine_sc_0 = scaler.transform(wine)\n",
        "wine_sc = pd.DataFrame(data = wine_sc_0, columns = wine.columns)\n",
        "wine_sc.head()"
      ],
      "metadata": {
        "id": "-G0i5P-VPhoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(wine_sc.corr(), annot = True)"
      ],
      "metadata": {
        "id": "he5iFu-nSI2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-Means Clustering\n",
        "For K-Means clustering, we choose the number of cluster centers (AKA wine types) and the model will automatically assign each wine to their group. THe way this model works is by choosing a data point as the \"center\" and picking data points closest to the center. The model will change centers until it realizes that it doesn't need to change centers anymore. That will be the \"final center\". We aren't sure about how many cluster centers we want so we can use a bar graph to evaluate this."
      ],
      "metadata": {
        "id": "dMMnxUwUpf-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph explains the total distance between each datapoint and its center. With only 1 center point, we see a very high sum of squared distance. We can't choose a bunch of centers, since this leads to overfitting. Each data point would be its own cluster center if we went this route. According to the graph, we seem to be okay with choosing 5 since the total distance doesn't drastically reduce after 5 centers."
      ],
      "metadata": {
        "id": "4zgyxJgqp7E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "wcss = []\n",
        "for i in range(1,13):\n",
        "  kmeans = KMeans(n_clusters= i, init = 'k-means++', max_iter = 1000, n_init = 10, random_state = 123)\n",
        "  kmeans.fit(wine_sc)\n",
        "  wcss.append(kmeans.inertia_)\n",
        "plt.bar(range(1,13), wcss)\n",
        "plt.title('Total Sum of Squares Within Each Cluster')\n",
        "plt.xlabel('# of Clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "1GX2ehQPP176"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model and fit the wine data\n",
        "kmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "kmeans.fit(wine_sc)\n",
        "centroids = kmeans.cluster_centers_\n",
        "#centroids"
      ],
      "metadata": {
        "id": "qur4rVZuQyfV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The array of values above indicates the coordinates for the cluster center. For example, the 1st cluster center's 1st number indicates the coordinate for the \"fixed acidity\" variable and so on. Let's predict each wine's cluster center and visualize it. "
      ],
      "metadata": {
        "id": "kQQ6OgQpqVzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = kmeans.fit_predict(wine_sc)\n",
        "label"
      ],
      "metadata": {
        "id": "RjoSybNZR665"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All rows of the wine data that belong to the 1st cluster center/centroid\n",
        "filtered_label0 = wine_sc[label == 0]\n",
        "filtered_label0.head(3)"
      ],
      "metadata": {
        "id": "CdriNhqadlgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The black dots indicate the centroid, while each color pertains to their own cluster. Based on the model we selected, it seems a little bit clobbered up. However, that is the point of K-means clustering. If it was obvious, we wouldn't need to run a machine learning model and just simply circle the groups by hand. Keep in mind we ran this model according to all 12 features. That implies a lot of variance and directions for our clustering algorithm to work with. Let's apply principal component analysis to reduce this dimensionality and make it easier for our algorithm to classify the wine data."
      ],
      "metadata": {
        "id": "OcDpegVTqzOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "num_clusters = np.unique(label)\n",
        "for i in num_clusters:\n",
        "  plt.scatter(wine_sc.iloc[label == i, 8], wine_sc.iloc[label ==i, 10], label = i)\n",
        "plt.scatter(centroids[:,8], centroids[:,10], s = 80, color = 'black')\n",
        "plt.xlabel('pH')\n",
        "plt.ylabel('Alcohol')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VNwmUwMlV44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Component Analysis\n",
        "Recall that we had to work with 12 features. This is a high-dimensional space, and helps reduce the noise for our algorithm for clustering. The principal components aid in accounting for the variance that occurs within our dataset."
      ],
      "metadata": {
        "id": "pjj2C_kVrQoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to choose the number of principal components that accounts for a lot of variation without going too high. Otherwise, we're back at 12 features. The cumulative sum of 4 principal components seem to account for about 80% of our variation, which I would say is good enough."
      ],
      "metadata": {
        "id": "djc2Huzwrn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "pca = PCA(n_components=10)\n",
        "pca_fit = pca.fit(wine_sc)\n",
        "pc_values = np.arange(pca_fit.n_components_) + 1\n",
        "plt.bar(pc_values, pca.explained_variance_ratio_)\n",
        "plt.title('Principal Component Analysis Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CgkRoUEseBFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 4)\n",
        "principalComponents = pca.fit_transform(wine_sc)\n",
        "pcDF = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2','PC3','PC4'])\n",
        "pcDF.head(2)"
      ],
      "metadata": {
        "id": "EfirNqL1WEUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this \"new dataset\", we can apply our Kmeans algorithm to see if our wine clustering looks more clean and appropriate."
      ],
      "metadata": {
        "id": "wX1aD-8ysL8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "kmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "label_pca = kmeans.fit_predict(pcDF)\n",
        "num_clusters_pca = np.unique(label_pca)\n",
        "centroids = kmeans.cluster_centers_\n",
        "for i in num_clusters_pca:\n",
        "  plt.scatter(pcDF.iloc[label == i, 0],pcDF.iloc[label == i, 1], label = i)\n",
        "plt.scatter(centroids[:,0], centroids[:,1], s = 80, color = 'black')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_o6A9i7RbJqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having our clustering algorithm work with the PC dataset definitely makes our graph look easier to distinguish which datapoint belongs to which cluster."
      ],
      "metadata": {
        "id": "K55BG4InsXNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "It seems that our clustering algorithm can work with either 4 or 5 clusters. We see the blue and purple datapoints overlap one another, so 4 would have worked. Considering that we have about 1600 rows, I thought it would be a better idea to increase to 5 clusters for interpetability. The yellow, green, and red data points are easily distinguishable from each other. This gives an additional perspective on our wine data. Recall that we tried to predict wine quality based on the other features. This time, we can understand the relationship between one another, so our leadership team can make some interpretations for each feature of the wine. In our principal component analysis, we find that 4 factors are good enough to explain the variation of the data. This allows us to work with only 4 columns instead of 12, reducing the high-dimensionality that makes it difficult for an algorithm to calculate. "
      ],
      "metadata": {
        "id": "WkCOJbSusUf_"
      }
    }
  ]
}