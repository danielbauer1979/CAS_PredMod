{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess1_RegressionAndBoostrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 1 -- Intro, Regression, Bootstrap\n",
        "Dani Bauer\n",
        "9/4/2022"
      ],
      "metadata": {
        "id": "HO7UGrGynGi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you will take first steps with programming in Py in the context of a bootstrap example. We will then demonstrate how to use the boostrapping more generally in the context of a risk measurement example. We will then review linear regression."
      ],
      "metadata": {
        "id": "N61KZ3fonYI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by importing some of the key libraries. Python by itself is pretty bare-bones, so we have to import most the functionality we required."
      ],
      "metadata": {
        "id": "4l4SqhWKnrUu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngbmVodjrNSm"
      },
      "source": [
        "import pandas as pd #the key statistical library\n",
        "import numpy as np #the key numerical library\n",
        "import math\n",
        "import matplotlib.pyplot as plt #one of the primary plotting libraries\n",
        "import statsmodels.api as sm #provides simple regression models, allows for formulas like R\n",
        "import scipy.stats as st"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plus we will clone our [github repository](https://github.com/danielbauer1979/CAS_PredMod) so as to have all the data available."
      ],
      "metadata": {
        "id": "WOZ03ac0oPZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI4VDNDTmQzA",
        "outputId": "e94cd255-50ef-467b-c7de-76d28a3cf67a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CAS_PredMod'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 120 (delta 18), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Receiving objects: 100% (120/120), 20.57 MiB | 10.47 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to Python via a Bootstrap Example"
      ],
      "metadata": {
        "id": "zz5hkAQ8pgMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some basic Python functionality"
      ],
      "metadata": {
        "id": "OllbacIzpqT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start out by generating $n=1,000$ [Gamma distributed](https://en.wikipedia.org/wiki/Gamma_distribution) random variables with parameters $\\alpha=3$ and $\\theta =100$. In a simulation exercise, it is always helpful to fix the seed for the random number generation such that the code is reproducible."
      ],
      "metadata": {
        "id": "WfBA3JMHpxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) #set seed\n",
        "\n",
        "alpha = 3\n",
        "theta = 100\n",
        "nsim = 1000\n",
        "\n",
        "x = np.random.gamma(alpha, theta, nsim)"
      ],
      "metadata": {
        "id": "NqhK8zfPqTXG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot a simple histogram via matplotlib."
      ],
      "metadata": {
        "id": "aBhfPYbRr7t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(x)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "czfM8znDrx3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's calculate some aggregate statistics"
      ],
      "metadata": {
        "id": "hWaKv0tasDP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.mean() #mean"
      ],
      "metadata": {
        "id": "4sOPA4RKr19r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.var() #variance"
      ],
      "metadata": {
        "id": "CmONtDSYsHN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we know the \"true\" expected value and variance here:"
      ],
      "metadata": {
        "id": "24XY1bAgtKyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Expected Value = \",alpha*theta,\"and Variance =\", alpha * theta * theta)"
      ],
      "metadata": {
        "id": "oeu-H4njtapn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and while the *estimates* are close, that's what they are: *estimates* based on a finite sample!"
      ],
      "metadata": {
        "id": "F96U2mystmW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boostrapping"
      ],
      "metadata": {
        "id": "RYrq8sm0sSqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boostrapping generally refers to sampling with replacement. That is, we take an original dataset and just generate a “new” version of the data by sampling from the original data.\n",
        "\n",
        "But why on earth would one want to do this? It’s simple: By sampling with replacement, we have a new data set that should have the same statistical properties as our original data set. Hence, we can obtain information on the uncertainty of quantities we estimate from the data by analyzing how they vary across our boostrap samples.\n",
        "\n",
        "Let’s consider this is the context of our previously simulated data set of Gamma random variables."
      ],
      "metadata": {
        "id": "QtAy_85atA4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import TRUE\n",
        "np.random.seed(123)\n",
        "mus = [0] * 500\n",
        "mus\n",
        "for i in range(500):\n",
        "  keep = np.random.choice(x, size=nsim, replace = TRUE)\n",
        "  mus[i] = keep.mean()"
      ],
      "metadata": {
        "id": "Vg7nl2fNt-3-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the mus now contain estimates of the mean for each boostrap sample, and by plotting them we will obtain information of how variable our estimate is:"
      ],
      "metadata": {
        "id": "5Zhv2GPwwQDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(mus)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xg1q57JXwGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our conventional confidence band for the mean is:"
      ],
      "metadata": {
        "id": "tueC9AZPwnMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"confidence band: [\",x.mean()-1.96*np.sqrt(x.var()/nsim),\",\",x.mean()+1.96*np.sqrt(x.var()/nsim),\"]\")"
      ],
      "metadata": {
        "id": "yHo7NzjXwtgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, and this is the beauty of the bootstrap, we can also obtain confidence intervals by just considering our boostrap distribution:"
      ],
      "metadata": {
        "id": "egDRCON8xSjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"bootstrapped confidence band: [\",np.quantile(mus, 0.025),\",\",np.quantile(mus, 0.975),\"]\")"
      ],
      "metadata": {
        "id": "ggrJMGVWxWiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping in Action: Estimates of Errors in Risk Measurement"
      ],
      "metadata": {
        "id": "8X-mS6L7x7Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A primer on Risk Measures"
      ],
      "metadata": {
        "id": "qhSXi6ixx_8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Risk Measures* have various applications in insurance and finance. For example, capital requirements for companies or certain lines of business are set via risk measures, and insurers use risk measures for capital allocation – which is a key input in pricing and performance measurement.\n",
        "\n",
        "Generally, the risk measure $\\rho$ for a random outcome $X$ is a number that depends on the distribution of $X,$ i.e. we are evaluating $\\rho(X)$. Here, $X$ can be given via a distribution or via a dataset, so that it may be more appropriate to write $\\rho(X_1,X_2,...,X_N)$ where $X_i$ are (independent and identically-distrubuted) realizations of the risk $X.$\n",
        "\n",
        "Prominent examples of risk measures are the Value-at-Risk (VaR) at a certain confidence level $\\alpha,$ say 1%. This corresponds to the 99% quantile, i.e. the number so that we can be sure that a given loss outcome will be smaller than VaR-1% with a probability of 99%. Another example is the Tail-Value-at-Risk (TVaR) -- or also called the Conditional Tail Expectation -- that averages the losses in the tail above the VaR at a given confidence level.\n",
        "\n",
        "Determining the VaR or the TVaR is straightforward when having a sample of losses. For VaR, all we need to do is sort the sample and then take the $\\alpha \\times N$ largest value. E.g., if we have 1,000 outcomes and we are looking for VaR-1%, we simply take the 10th largest value. Similarly, for TVaR, we average over the outcomes in the tail. In the context of the example, that amounts to taking the average of the ten largest outcomes.\n",
        "\n",
        "However, due to the direct correspondence to capital, companies as well as regulators are not only interested in the actual number, but also in the associated uncertainty. For instance, if the estimate is 10 million USD, is it really something between 5 million or 15 million – or something between 9 million and 11 million? In particular, stakeholders frequently are interested in confidence bands for the risk measure.\n",
        "\n",
        "While generally a nontrivial probabilistic problem, the bootstrap allows for a relatively straightforward assessment of the estimation uncertainty – or, in other words, the calculation of a confidence interval:\n",
        "\n",
        "* Draw $B$ samples of size $N$ from the original dataset $X_1,...,X_N.$\n",
        "* For each of the $B$ new datasets, evaluate the risk measure $\\rho$.\n",
        "* Plot all $B$ estimates or determine large and small choices to assess the uncertainty. For instance, a 90% confidence region is given by the $5\\%×B$ largest value and the $95\\%×B$ largest value."
      ],
      "metadata": {
        "id": "JhiXS9JmyD1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application in the Context of Norwegian Fire Insurance Claims"
      ],
      "metadata": {
        "id": "IgpVmJ5lo0Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take the Norwegian fire loss data from the textbook on extremal modeling by Jan Beirlant, Yuri Goegebeur, Johan Segers, Jozef L. Teugels. The 2006 North-American Actuarial Journal paper by Thomas Kaiser and Vytaras Brazauskas carries out a similar exercise as the one considered here. The dataset is saved on our github repository, so you can access it with ease.\n",
        "\n",
        "Thwe dataset is tab separated and does not have columns"
      ],
      "metadata": {
        "id": "dk26VkzBo9ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_data = pd.read_csv(\"CAS_PredMod/pa_data_norwegianfire.txt\",delimiter='\\t',header=None)\n",
        "fire_data.columns = ['Loss', 'Year']\n",
        "n_fire_data = len(fire_data.index)\n",
        "fire_data.head()"
      ],
      "metadata": {
        "id": "94ybM91qo4vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As described, calculating risk measures is relatively straightforward when sorting the losses:"
      ],
      "metadata": {
        "id": "H1bNefFGqzGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire_data_sort = fire_data.sort_values(by=['Loss'])\n",
        "fire_data_sort = fire_data_sort.reset_index(drop=True)\n",
        "Losses = fire_data_sort.Loss\n",
        "VaR1P = Losses[np.floor(.99 * n_fire_data)-1]\n",
        "VaR1P"
      ],
      "metadata": {
        "id": "X1-ccoE-qxIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TVaR1P = Losses[range(math.floor(.99 * n_fire_data)-1,n_fire_data)].mean() #the np floor function does not return an integer, so using the math version\n",
        "TVaR1P"
      ],
      "metadata": {
        "id": "hmum9GeBv7un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the estimation error, we again rely on bootstrapping. We proceed as before:"
      ],
      "metadata": {
        "id": "2m4mv0oextDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "VaRs = [0] * 500\n",
        "TVaR1P = [0] * 500\n",
        "mus\n",
        "for i in range(500):\n",
        "  keep = np.random.choice(Losses, size=n_fire_data, replace = TRUE)\n",
        "  VaRs[i] = np.quantile(keep,0.99)"
      ],
      "metadata": {
        "id": "DqRf2lSS0qGy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can illustrate the uncertainty in our estimate via looking at tge resulting distribution of our estimates."
      ],
      "metadata": {
        "id": "ecRhoBGM2ubj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(VaRs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hOJTvRvW2fK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the following confidence band:"
      ],
      "metadata": {
        "id": "4MVlPdvr22FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"bootstrapped confidence band: [\",np.quantile(VaRs, 0.025),\",\",np.quantile(VaRs, 0.975),\"]\")"
      ],
      "metadata": {
        "id": "bIrlJSo22BDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also provided functionality within Python. E.g., we can use the boostrap function within the scipy stats library to obtain a confidence interval for the median:"
      ],
      "metadata": {
        "id": "tZJt13nz12ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Losses_list = (Losses,)\n",
        "bootstrap_ci = st.bootstrap(Losses_list, np.median, confidence_level=0.95,random_state=1, method='percentile')\n",
        "bootstrap_ci"
      ],
      "metadata": {
        "id": "I0rhHglkulcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Refresher: Galton Data"
      ],
      "metadata": {
        "id": "uwifWl1o3fvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most basic predictive modeling technique is (linear) regression. To provide a refresher, let’s look at the famous Galton data example which provides data on the heights of parents and their children (the data needs to be in your working directory):"
      ],
      "metadata": {
        "id": "nCA0tOEj3lJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "galton_data = pd.read_csv(\"CAS_PredMod/pa_data_galtonData.txt\")\n",
        "galton_data.head()"
      ],
      "metadata": {
        "id": "JczWmoQ03mBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsPzBXbOrNSq"
      },
      "source": [
        "galton_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review of Linear Regression\n",
        "\n",
        "In a conventional linear regression problem with independent variables $x_i$ and dependent variables $y_i$, we assume a linear relationship:\n",
        "$$\n",
        "y_i = \\beta_0+\\sum_{j=1}^p \\beta_j\\,x_{i,j} + \\varepsilon_i.\n",
        "$$\n",
        "\n",
        "In order to make a *prediction* for a data point $x_0$ outside of our sample -- which is the purpose of the entire enterprise -- we need to determine an *estimate* for our regression parameter $\\beta$, which is generally labeled as $\\hat{\\beta}$ .  We will use our data set -- the so-called *training data* -- for deriving this estimate.  Once we have an estimate $\\hat{\\beta}$, we obtain a prediction for our (unknown) outcome for the *features* $x_0$ via:\n",
        "$$\n",
        "y_0 = \\hat{\\beta}_0 + \\sum_{j=1}^p \\hat{\\beta}_j\\,x_{0,j}.\n",
        "$$\n",
        "\n",
        "\n",
        "OLS regression determines the estimate $\\hat{\\beta}$ that best approximates the training data in the *least-squares sense*:\n",
        "$$\n",
        "\\hat{\\beta}^{\\text{OLS}} = \\text{argmin}_{\\beta}\\left\\{\\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j\\,x_{i,j}\\right)\\right)^2 \\right\\}.\n",
        "$$\n",
        "The OLS estimate has some nice properties, e.g.:\n",
        "\n",
        "* *Computational*: It is possible to solve for the OLS estimate in closed form.  Indeed, we obtain:\n",
        "$$\n",
        "\\hat{\\beta}^{\\text{OLS}} = (X'X)^{-1}X'y,\n",
        "$$\n",
        "where $X$ is the so-called *design matrix*, whose row $i$ is $x_i'$ and contains the features for the $i$th observation.\n",
        "\n",
        "* *Statistical*: Due to the estimator's form, it is possible to derive the (approximate/limiting) distribution of the estimate via the central limit theorem.  In particular, this allows to draw inference whether the influence is significantly different from zero (illustrated via \"stars\", see below)."
      ],
      "metadata": {
        "id": "JxiukKM65L8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many packages for running regressions in Python. Here we rely on a package that allows for forumulas and thus has a similar feel as R regressions. We will get to know others soon:"
      ],
      "metadata": {
        "id": "_SbQoACX4IkW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRs0SterNSq"
      },
      "source": [
        "from statsmodels.formula.api import ols\n",
        "first_reg = ols(formula=\"child ~ parent\", data=galton_data).fit()\n",
        "print(first_reg.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get confidence intervals for the regression coefficients:"
      ],
      "metadata": {
        "id": "DGI-PWKw5tqj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RIAeFgprNSr"
      },
      "source": [
        "first_reg.conf_int(alpha=0.05, cols=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the residulas to check whether they are exhibiting an adequate form (ideally, theyb should be close to a normal distribution)."
      ],
      "metadata": {
        "id": "jHUpOVRc6rp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = galton_data['child']\n",
        "y_pred = first_reg.predict()\n",
        "eps = y - y_pred\n",
        "eps.plot(kind='hist')"
      ],
      "metadata": {
        "id": "oi_LEwvL5_41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check more formally via a q-q plot:"
      ],
      "metadata": {
        "id": "ZNGuOCQK6-Vj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFZGm4cErNSw"
      },
      "source": [
        "fig = sm.qqplot(eps, line=\"s\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}