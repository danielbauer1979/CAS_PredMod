{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess5_NonLinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Non-linear Regression Techniques and Generalized Additive Models\n",
        "\n",
        "Daniel Bauer, 2022\n",
        "\n",
        "In this tutorial, we will get acquainted with non-linear models.  In particular, we introduce a number of non-linear regression techniques -- including polynomial regression, regression splines, smoothing splines, and local regression -- in a simple example setting.  The second part focuses on Generalized Linear Models -- or GAMs."
      ],
      "metadata": {
        "id": "JNGZbbCiRkR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the techniques we introduce don't exist in the packages that come with Colab. So we will install two additional libraries: scikit-lego for local regression and pygam for GAMs."
      ],
      "metadata": {
        "id": "6xa9J22nR22l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-lego\n",
        "!pip install pygam"
      ],
      "metadata": {
        "id": "nE9d5Kpg7Az7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that, let's install the relevat libraries."
      ],
      "metadata": {
        "id": "DvC0N_VKSFaw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yZYbDPGZBq"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import SplineTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklego.linear_model import LowessRegression\n",
        "\n",
        "from patsy import dmatrix\n",
        "from pygam import LinearGAM, LogisticGAM, GAM, s, f, l\n",
        "\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And clone our git repository so as to have access to the data."
      ],
      "metadata": {
        "id": "2PRV7QtLSLCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7rMHNGajOvV"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear Regression Models\n",
        "\n",
        "### Primer on Non-linear Regression Techniques\n",
        "\n",
        "Non-linear models expand on the more foundational models that we discussed before, particularly linear regression.  We consider three different though related categories of approaches, which are relatively straightforward in the context of a single feature, i.e. if there is only one $x$.\n",
        "\n",
        "1. Use *transformations* of $x$: A traditional approach that falls in this category is *polynomial regression*, where we simply add powers of the feature $x$ -- i.e. $x^2$, $x^3$, etc. -- to the regression problem.  However, polynomials have some limitations, particularly in extremal areas, because they fit the regression function globally.  Instead, in *spline regression*, piecewise polynomials that are only fit between *knots* are used, but one imposes restrictions such that the fit is still continuous and smooth.  With so-called *natural splines*, a linear function is used in the extremal (corner) areas so as to avoid erratic behavior when extrapolating.  Depending on the number of knots, the function can mimic more or less arbitrary shapes.\n",
        "\n",
        "2. Using an arbitrary function but *penalizing* said function for variation.  This yields a so-called *smoothing spline* (so the approach is related to 1.), but it also depends on a smoothing parameters that governs how much variation is allowed.  Similar to regularized methods, this parameter can be used to control *model complexity*.\n",
        "\n",
        "3. *Local regression*: Instead of using all points for predicting at a given point $x_0$, we run a *local* regression where we put more weight on the data points that are close to $x_0$ and less weight on data points that are far away.  The weighting is typically done via a so-called *kernel* function (generalized bell curve) so this is also referred to as *kernel regression*.\n"
      ],
      "metadata": {
        "id": "Ilfi7DvlSXsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example in the context of height-weight relationship\n",
        "\n",
        "We use a straightforward example: Regressing the weight of people on their heights. The relevant dataset is taken from a [well known example dataset from Davis (1990)](https://rdrr.io/cran/carData/man/Davis.html):"
      ],
      "metadata": {
        "id": "iL_1gPeUSg9v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcUhqqpbGZBr"
      },
      "source": [
        "hwdata2 = pd.read_csv('CAS_PredMod/pa_data_davis.csv', index_col=0) \n",
        "hwdata = hwdata2.sort_values('height')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "JoeN5uvIT6a-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQcuj4taGZBs"
      },
      "source": [
        "hwdata2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's defined the $x$ and $y$ vectors:"
      ],
      "metadata": {
        "id": "EHWdL7HEUJwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = hwdata2[['height']]\n",
        "y = hwdata2['weight']"
      ],
      "metadata": {
        "id": "zENkLwAFUOmA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with polynomial regression, where we are taking advantage of so-called *model pipelines* within scikit-learn, where we \"pipe\" the regression model into our linear regression model:"
      ],
      "metadata": {
        "id": "QN6VXn_OT8Uv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orzorYDJjuTO",
        "outputId": "f05a5018-a6f8-4ae7-e84d-4aaa1332a89a"
      },
      "source": [
        "polynomial_regression = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', LinearRegression(fit_intercept=False))])\n",
        "polynomial_regression = polynomial_regression.fit(X, y)\n",
        "polynomial_regression.named_steps['linear'].coef_"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.57931469e+04,  1.10301542e+03, -9.93090159e+00,  3.96257277e-02,\n",
              "       -5.90686627e-05])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize via a scatter plot with polynomial regression line:"
      ],
      "metadata": {
        "id": "i4vks-QWUfpO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bk_XF8GZBt"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_poly = polynomial_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='g', label='polynomial regression df=4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we may be overfitting a bit...\n",
        "\n",
        "Let's look at regression splines, again using a model pipeline (we get splines via 'SplineTransformer'):"
      ],
      "metadata": {
        "id": "vJ4RsocQUoEd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2thG-dVqZFt"
      },
      "source": [
        "spline_regression = Pipeline([('splines', SplineTransformer(degree=2, n_knots=3, extrapolation='linear')),('linear', LinearRegression(fit_intercept=False))])\n",
        "spline_regression = spline_regression.fit(X, y)\n",
        "spline_regression.named_steps['linear'].coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a spline regression line:"
      ],
      "metadata": {
        "id": "gdHCaKuLU91L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Vh7s5qvNfg"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_splines = spline_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_splines, color='g', label='Spline regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "Finally, let's run a local regression, where we rely on 'LowessRegression' from scikit-lego:"
      ],
      "metadata": {
        "id": "4Llw3Eg6VGY7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIdO1yQYv6gN"
      },
      "source": [
        "local_regression = LowessRegression(sigma=50).fit(X,y)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a regression line:"
      ],
      "metadata": {
        "id": "hlt2l-X-VR8b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5WlCKnxSHL"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_local = local_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_local, color='g', label='local regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "So the moral of the story is that these techniques present very neat tools for modeling non-linear relationships. \n",
        "\n",
        "Let's summarize by plotting altogether:"
      ],
      "metadata": {
        "id": "YML989SFVZZN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znbTtt-dGZBx"
      },
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='r', label='polynomial regression df=4')\n",
        "plt.plot(hwdata.height, pred_splines, color='b', label='Natural spline df=4')\n",
        "plt.plot(hwdata.height, pred_local, color='y', label='Local regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Additive Models\n",
        "\n",
        "As depicted, non-linear regression models are powerful approaches for depicting non-linear relationships -- with the key caveat that our explanatory variable had a single dimension only.  \n",
        "\n",
        "One learner -- so-called **Generalized Additive Models** (GAMs) -- leverages the performance of non-linear regression models in lower dimensions but imposes an *additive* structure between the functions of the individual features:\n",
        "$$\n",
        "g\\left(\\mathbb{E}\\left[Y | X_1,...,X_p\\right]\\right) = \\alpha + f_1(X_1) + \\ldots + f_p(X_p).\n",
        "$$\n",
        "Hence, in case of a basic regression problem where $g(x)=x$ we have the special case of *Additive Models*:\n",
        "$$\n",
        "Y = \\alpha + f_1(X_1) + \\ldots + f_p(X_p) +\\varepsilon.\n",
        "$$\n",
        "As such, GAMs take on an intermediate position between linear regression and a general non-linear model:  They generalize the impact each feature can have on the outcome, but they keep the same structure on their relationship.  In particular, they do not allow for rich interactions between the variables -- which is the key downside of GAMs.\n",
        "\n",
        "###Wage Regression Example"
      ],
      "metadata": {
        "id": "CvX5H1onVvQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how GAMs work in a relevant example, let us consider the wage regression example from the texbook [ISLR](https://www.statlearning.com/).  Here, we model wages as a function of the year, age, and education level:\n",
        "$$\n",
        "\\text{wage}_i = \\alpha + f_1(\\text{year}) + f_2(\\text{age}) + \\text{education level}_i + \\varepsilon\n",
        "$$"
      ],
      "metadata": {
        "id": "pAfYvPCsWBjH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4JW35mTGZBx"
      },
      "source": [
        "mydata = pd.read_csv('CAS_PredMod/pa_data_wage.csv', index_col=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Pygam package. Python requires mannually dummifying categorical variables for this algorithm."
      ],
      "metadata": {
        "id": "ikixBtISWXxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_ = mydata[['year','age']]\n",
        "dummies = pd.get_dummies(mydata['education'], drop_first=True)\n",
        "X = pd.concat([X_, dummies], axis = 1)\n",
        "y = mydata['wage']"
      ],
      "metadata": {
        "id": "Kybek2AF9wzz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to specify the model, deciding whether we want to treat the different variables as spline (s), factor (f), linear (l), etc)"
      ],
      "metadata": {
        "id": "IP-XTHgTWf-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gam = LinearGAM(s(0,n_splines=6)+s(1,n_splines=7)+f(2)+f(3)+f(4)+f(5)).fit(X, y)\n",
        "gam.summary()"
      ],
      "metadata": {
        "id": "MXPmhJhm9z3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize, where remember we dummified the \"education\" variable into four indicators:"
      ],
      "metadata": {
        "id": "unHbAeGEWy7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (28, 8)\n",
        "fig, axs = plt.subplots(1, 6)\n",
        "titles = list(X.columns)\n",
        "for i, ax in enumerate(axs):\n",
        "  XX = gam.generate_X_grid(term=i)\n",
        "  ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "  ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1],c='k', ls='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DePNe_nr99im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get presictions as:"
      ],
      "metadata": {
        "id": "Y4B7XypBW7UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = gam.predict(X)"
      ],
      "metadata": {
        "id": "2SuVbX9e-Nny"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One neat aspect of this packages is that we are considering **G**AMs, i.e. we can define alternate link functions or distributions. Let's do a logistic GAM, where our $y$ variable is a one/zero variable with an indicator of whether the wage is above \\$250."
      ],
      "metadata": {
        "id": "MEG3te5fW9vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mydata[['year','age']]\n",
        "y = mydata['wage'] > 250"
      ],
      "metadata": {
        "id": "O6Pw1UAV-UOB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can control the hyper-parameters n_splines for splines and lambdas for the penalty."
      ],
      "metadata": {
        "id": "NsDMh94NXiE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gam5 = LogisticGAM(l(0)+s(1, n_splines=50), lam=0.6).fit(X, y.values.astype(int))\n",
        "gam5.summary()"
      ],
      "metadata": {
        "id": "YqSgTRMjXoBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the results:"
      ],
      "metadata": {
        "id": "sRM8vkWNXq-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (28, 8)\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "titles = list(X.columns)\n",
        "for i, ax in enumerate(axs):\n",
        "  XX = gam5.generate_X_grid(term=i)\n",
        "  ax.plot(XX[:, i], gam5.partial_dependence(term=i, X=XX), c='g')\n",
        "  ax.plot(XX[:, i], gam5.partial_dependence(term=i, X=XX, width=.95)[1],c='k', ls='--')\n",
        "ax.set_title(titles[i]);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-umY1AVJ-Y8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}