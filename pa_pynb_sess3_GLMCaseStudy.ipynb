{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess3_GLMCaseStudy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Session 3 -- GLM Case Study\n",
        "\n",
        "Dani Bauer, 9/12/2022\n",
        "\n",
        "In this tutorial, we will present a detailed case study in the context of auto liability insurance, showcasing a variety of GLM techniques.\n",
        "\n",
        "Let's start by loading the libraries that are going to be helpful. Again, we're going to rely on the statistical learning toolkit ski-cit learn, which provides GLM functionalty but also will be used in the context of algorithmic learners. It is less comfortable to use than some of the other packages and, unlike R, does not support formulas. But it is versatile and fast, and therefore one of the most popular prdictive modeling toolkits."
      ],
      "metadata": {
        "id": "gYTM3oGIj6tD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKTT21YGUgFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL8OZzdpMbDG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "from sklearn.linear_model import GammaRegressor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider predictive modeling of auto claims, i.e. the overarching challenge is predicting frequencies and/or severities of claims.  We rely on the comprehensive French Motor Third-Part Liability datasets `ferMTPLfreq` and `ferMTPLsev` available within the [package CASdatasets](http://cas.uqam.ca/).\n",
        "\n",
        "Let's take a peak, first at the frequency dataset:"
      ],
      "metadata": {
        "id": "meSC5JMRkagB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bglok6-BCHZ"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUQXZq64MbDI"
      },
      "source": [
        "dat_frq1 = pd.read_csv('CAS_PredMod/pa_data_freMTPLfreq1.csv')\n",
        "dat_frq2 = pd.read_csv('CAS_PredMod/pa_data_freMTPLfreq2.csv')\n",
        "dat_frq = pd.concat([dat_frq1,dat_frq2])\n",
        "dat_frq.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npw581FNMbDJ"
      },
      "source": [
        "dat_frq.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e__rQGGOEfK7"
      },
      "source": [
        "pd.crosstab(index=dat_frq['ClaimNb'], columns=\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsQ6OdJqMbDK"
      },
      "source": [
        "pd.crosstab(index=dat_frq['ClaimNb'], columns=\"count\").plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, as expected, multiple claims are rare. The vast majority of cases don't have a claim.\n",
        "\n",
        "Let's look at the severities:"
      ],
      "metadata": {
        "id": "aQtsK-vWkr5p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hejS5buBMbDL"
      },
      "source": [
        "dat_sev = pd.read_csv('CAS_PredMod/pa_data_freMTPLsev.csv')\n",
        "print(dat_sev.shape)\n",
        "dat_sev.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Usuf8QDMbDM"
      },
      "source": [
        "dat_sev.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, again, as expected, we have a few very large claims."
      ],
      "metadata": {
        "id": "g1jiPiTElHih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the Data Sets\n",
        "\n",
        "Since there are multiple claims for each policy, let's summarize the claims to the policy level, so as to allow for an easy merge:"
      ],
      "metadata": {
        "id": "_ibmOqUTlMlk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGtRhLmeMbDN"
      },
      "source": [
        "df = dat_sev.groupby('PolicyID', as_index=False).agg({\"ClaimAmount\":\"mean\"})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can simply merge the frequency and the severity sets into our master data set, where we set the `NA` `ClaimAmount` entries to zero where we don't have claims:"
      ],
      "metadata": {
        "id": "TNFSHGzclsBf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pga7xTJlMbDO"
      },
      "source": [
        "dat = pd.merge(dat_frq, dat_sev.groupby('PolicyID', as_index=False).agg({\"ClaimAmount\":\"mean\"}),how='left')\n",
        "dat = dat.fillna(0)\n",
        "dat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Models\n",
        "\n",
        "As the previous time, we need to put the categorical variables to dummies. For that, we separate the dummies and the numerical variables, make the dummies, and then concatenate."
      ],
      "metadata": {
        "id": "LuHNeCFBlw9o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGomSaWpG_bl"
      },
      "source": [
        "dummies = pd.get_dummies(dat[dat.columns[[4,7,8,9]]],drop_first=True)\n",
        "dat = dat.drop(dat.columns[[0, 1, 4, 7, 8, 9]], axis=1)\n",
        "dat = pd.concat([dat,dummies], axis=1)\n",
        "dat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do some visualizations of some of the variables:"
      ],
      "metadata": {
        "id": "dPlQ7vLNmV9D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLk-Pkk6MbDP"
      },
      "source": [
        "plt.hist(dat['Exposure'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR7Moc-3MbDQ"
      },
      "source": [
        "plt.hist(dat['DriverAge'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...likely what we expected..."
      ],
      "metadata": {
        "id": "yRrdK8pXmbAx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06d-HUe8MbDR"
      },
      "source": [
        "plt.hist(dat['CarAge'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...also no surprises...\n",
        "\n",
        "Let's look at population density, that's a more interesting variable:"
      ],
      "metadata": {
        "id": "rcf3qxJNmflv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdrgJHVNMbDS"
      },
      "source": [
        "plt.hist(dat['Density'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like very small and a few very high densities. Let's go to log-scale."
      ],
      "metadata": {
        "id": "jdnMknGMmnMo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q1ivoVPMbDS"
      },
      "source": [
        "dat.loc[:, 'Density'] = np.log(dat['Density'])\n",
        "plt.hist(dat['Density'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check out our targets:"
      ],
      "metadata": {
        "id": "V01SE4mvm3LO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C8k2NeuMbDT"
      },
      "source": [
        "dat.loc[dat['ClaimAmount']>0,'ClaimAmount'].quantile([.9, .95, .99, .999])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible that the few very large claims will cause trouble, so let's cut off at 50K:"
      ],
      "metadata": {
        "id": "Tp2_MWppnAkp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSAP0adMbDU"
      },
      "source": [
        "dat['ClaimAmount'][dat['ClaimAmount']>50000] = 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split into a training and a test sample, so that we can evaluate our model:"
      ],
      "metadata": {
        "id": "WhfVK2DPnN7U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO9-A2vSIWYE"
      },
      "source": [
        "Train, Test = train_test_split(dat, test_size=0.25)\n",
        "Train_y_freq = Train['ClaimNb']\n",
        "Train_y_sev = Train['ClaimAmount']\n",
        "Train_X = Train.drop(columns = ['ClaimNb','ClaimAmount'])\n",
        "Test_y_freq = Test['ClaimNb']\n",
        "Test_y_sev = Test['ClaimAmount']\n",
        "Test_X = Test.drop(columns = ['ClaimNb','ClaimAmount'])\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's model frequencies via a Poisson Regression:"
      ],
      "metadata": {
        "id": "JN1y97tDne85"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjSS_F-lKOxO",
        "outputId": "1d97b6c3-306d-4026-de0a-55ee6e6d2477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "freqmodel = PoissonRegressor()\n",
        "freqmodel.fit(Train_X,Train_y_freq)\n",
        "preds_Train = freqmodel.predict(Train_X)\n",
        "np.corrcoef(preds_Train,Train_y_freq)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.01765559],\n",
              "       [0.01765559, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation is fairly low, but when we look at a scatter plot..."
      ],
      "metadata": {
        "id": "CQQVog1fnrre"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqWyDw-MniH"
      },
      "source": [
        "plt.scatter(Train_y_freq,preds_Train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it does seem like that the claims with higher frequencies have higher predictions, though they are still close to zero:"
      ],
      "metadata": {
        "id": "pAqmuQHrn7qq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHyZwloPL8ZK"
      },
      "source": [
        "preds_Test = freqmodel.predict(Test_X)\n",
        "np.corrcoef(preds_Test,Test_y_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems the positive correlation sustains in the test set (out of sample), and the scatter plot:"
      ],
      "metadata": {
        "id": "feY8TN9YoJbK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0CaB_alLXTG"
      },
      "source": [
        "plt.scatter(Test_y_freq,preds_Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also suggests the model isn't crazy.\n",
        "\n",
        "Let's looks at claims:"
      ],
      "metadata": {
        "id": "CnuEZkfLoRTN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h18_spWDMbDW"
      },
      "source": [
        "plt.hist(dat['ClaimAmount'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and non-zero claims on a log-scale."
      ],
      "metadata": {
        "id": "aVZLmKEnoXY7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AzVYDaeMbDW"
      },
      "source": [
        "plt.hist(np.log(dat.loc[dat['ClaimAmount']>0,'ClaimAmount']))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a Gamma regression for the severities:"
      ],
      "metadata": {
        "id": "CcFGWycAob2g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QVwGKbcNAT8"
      },
      "source": [
        "Train = Train.loc[Train['ClaimAmount']>0,:]\n",
        "Train_y_sev = Train['ClaimAmount']\n",
        "Train_X = Train.drop(columns = ['ClaimNb','ClaimAmount'])\n",
        "sevmodel = GammaRegressor()\n",
        "sevmodel.fit(Train_X,Train_y_sev)\n",
        "plt.scatter(Train_y_sev,sevmodel.predict(Train_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we're kind-of cathing the trend.\n",
        "\n",
        "Now we can fuse together by muliplying predicted frequencies and severities:"
      ],
      "metadata": {
        "id": "x3swOb71oplI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNfxc8OjP06W"
      },
      "source": [
        "preds_Test_sev = sevmodel.predict(Test_X)\n",
        "preds_Test_tot = preds_Test * preds_Test_sev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGRhWTucQVNh"
      },
      "source": [
        "plt.scatter(Test_y_freq * Test_y_sev,preds_Test_tot)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}