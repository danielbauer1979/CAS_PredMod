{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8MjzZHWGgOx+3MaMs/Hq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess9_AlgFairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 9: Algorithmic Bias and Fairness\n",
        "\n",
        "Jim Guszcza and Dani Bauer, 10/2023"
      ],
      "metadata": {
        "id": "Ccw0SuBI5LYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we discuss approaches how to analyze a predictive algorithm with regards to \"fairness.\" We do so in the context of a well-known case study on an algorithm that assists judges in parole decisions. We go over different notions of fairness, discuss tradeoffs, and explain the intuition behind the results of the analyses. We also discuss approaches how to adjust algorithms to enforce fairness. A second case study, which we will ask you to work on, illustrates how these ideas apply in the actuarial context.\n",
        "\n",
        "### Load required packages"
      ],
      "metadata": {
        "id": "zpUgxOcS5NiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kSv8hx144fah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas\n",
        "#Another library that seems to be popular is fariness 360:\n",
        "#!pip install aif360"
      ],
      "metadata": {
        "id": "BrPUeW1L6Xa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aequitas.group import Group\n",
        "from aequitas.bias import Bias\n",
        "from aequitas.fairness import Fairness\n",
        "import aequitas.plot as ap"
      ],
      "metadata": {
        "id": "EsxbkyTO6by5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compas Case Study\n",
        "\n",
        "The Compas data originates from a [well-known case study on algorithmic bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). The background is [as follows](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm): Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant's likelihood of becoming a recidivist---a term used to describe criminals who re-offend. One of the commercial tools made by Northpointe, Inc. is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). The case study compares outcomes and risk scores for individuals belonging to different races. In what follows, we will go through some of these analyses ourselves.\n",
        "\n",
        "## The Compas Data\n",
        "\n",
        "The data is in our github folder. Let's take a look:"
      ],
      "metadata": {
        "id": "YB2Fhuab_xpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "metadata": {
        "id": "XjM0xW_pATMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv('CAS_PredMod/pa_data_compasdata.csv')\n",
        "dat.head()"
      ],
      "metadata": {
        "id": "2HhtisuQAi3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat.describe()"
      ],
      "metadata": {
        "id": "sR0ZFQweBLV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains information on recidivism of 6,172 individuals as well as information on the individual's age, sex, criminal history, their ethnicity---and the risk score they received from the COMPAS algorithm.\n",
        "\n",
        "To simplify the situation, we focus on two ethnicity levels only: African-Americans and Caucasians."
      ],
      "metadata": {
        "id": "4hYinYNND79l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = dat.loc[(dat['ethnicity'] == 'Caucasian') | (dat['ethnicity'] == 'African_American')]\n",
        "dat.describe()"
      ],
      "metadata": {
        "id": "iE06ij2sD-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we still have the majority of individuals included. We will consider the African-Americans as the \"protected\" class.\n",
        "\n",
        "We commence by exploring the data some and looking at fairness manually, but then we will also explore how to use the Aequitas package in this setting."
      ],
      "metadata": {
        "id": "M696IiNCEyOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness scores vs. recidivism rates\n",
        "\n",
        "Let's start by comparing the COMPAS scores between the two ethic groups (see also the density plots from the fairness package above):"
      ],
      "metadata": {
        "id": "sM_2m9OIGRM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat.loc[dat['ethnicity'] == 'Caucasian']['probability']"
      ],
      "metadata": {
        "id": "xMla0FgbIQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Score distribution by group\")\n",
        "plt.xlabel(\"Group: 1 = Caucasian, 2 = African-American\")\n",
        "plt.boxplot([dat.loc[dat['ethnicity'] == 'Caucasian']['probability'],dat.loc[dat['ethnicity'] == 'African_American']['probability']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WH65Ol1QE2X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of scores in the African-American group has a higher median and higher percentiles than the scores in the Caucasian group.\n",
        "\n",
        "However, consider the number of re-offenders between the group, we see the following:"
      ],
      "metadata": {
        "id": "1cW5E765ON6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "by_race = sns.countplot(x=\"ethnicity\", hue=\"Two_yr_Recidivism\", data=dat[dat.ethnicity.isin(['African_American', 'Caucasian'])], palette=aq_palette)"
      ],
      "metadata": {
        "id": "pMMz2A__PzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider raw averages of re-offenders in the two groups -- for caucasians:"
      ],
      "metadata": {
        "id": "-Agc5sMDP3If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "qI7OQvUNOqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for the protected group:"
      ],
      "metadata": {
        "id": "NnnHTO18PRS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "W7denUplOgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can interpret the higher average COMPAS scores as resulting from statistical facts in this particular population.\n",
        "\n",
        "Let’s investigate accuracy, i.e., how accurate the algorithm is, on aggregate and by protected vs. non-protected class. In doing so, we determine the correctly classified re-offenders (true positives), the correctly cassified non-re-offenders (true negatives), the incorrectly classified re-offenders (false negatives), and the incorrectly classified non-re-offenders (false positives)—by class."
      ],
      "metadata": {
        "id": "e7LIq6T2PT3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "TP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * dat.loc[dat['ethnicity'] == 'African_American']['predicted'])\n",
        "FP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "FP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * dat.loc[dat['ethnicity'] == 'African_American']['predicted'])\n",
        "FN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "FN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * (dat.loc[dat['ethnicity'] == 'African_American']['predicted'] == 0))\n",
        "TN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "TN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * (dat.loc[dat['ethnicity'] == 'African_American']['predicted'] == 0))"
      ],
      "metadata": {
        "id": "E-VVrFQk9AC9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Accuracy\" now is simply the correctly classified individuals divided by all individuals."
      ],
      "metadata": {
        "id": "qiMjSDz-_9i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = (TP_0+TP_1+TN_0+TN_1)/(TP_0 + FP_0 + TP_1 + FP_1 + TN_0 + FN_0 + TN_1 + FN_1)\n",
        "acc"
      ],
      "metadata": {
        "id": "l-YDztCj_Yb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and by group:"
      ],
      "metadata": {
        "id": "919XS_aH_-jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 = (TP_0+TN_0)/(TP_0 + FP_0 + TN_0 + FN_0)\n",
        "acc_0"
      ],
      "metadata": {
        "id": "FYJy06F8ABDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and for the protected group:"
      ],
      "metadata": {
        "id": "B7eWfGckAa74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_1 = (TP_1+TN_1)/(TP_1 + FP_1 + TN_1 + FN_1)\n",
        "acc_1"
      ],
      "metadata": {
        "id": "XITH9iGiAege"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(['Caucasian','African-American'], [acc_0,acc_1], color ='maroon', width = 0.4)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy:    (TP+TN) / (TP+FP+TN+FN)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CPxs79d8AykW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the algorithm is similarly \"accurate\" for both groups, and in fact the accuracy is even somewhat higher for the protected group.\n",
        "\n",
        "Hence, one may conclude that the algorithm performs reasonably well---and that there are no major concerns with regards to differential performance for the protected group. In fact, this may have been the mindset by the creators of the COMPAS algorithm.\n",
        "\n",
        "### Alternative Fairness Metrics\n",
        "\n",
        "While \"accuracy\"---or one minus accuracy, which is the misclassification rate---do not point to problems, we can investigate more differentiated error metrics. One obvious way of doing this is by comparing\n",
        "\n",
        "We can further assess the performance by visualizing false negative rates and the true positive rates by group. One way of looking at this is how many of the negatively classified individuals (low risk) were labeled so incorrectly, and how many of the positively classified (high risk) were labeled so correctly. The former proportion of negatively classified for which outcome indeed was one is also referred to as the *False Omission Rate*, whereas the latter proportion of positively classified that were indeed correct is also referred to as *Precision*:"
      ],
      "metadata": {
        "id": "hqa9ZKM2Advy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['False Omission Rate',FN_0/(TN_0+FN_0),'Caucasian'],['False Omission Rate',FN_1/(TN_1+FN_1),'African_American'],['Precision',TP_0/(TP_0+FP_0),'Caucasian'],['Precision',TP_1/(TP_1+FP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette)"
      ],
      "metadata": {
        "id": "yBztwEiXFvu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start seeing some differences between the groups, although the differences are not startling. The algorithm seems to me more “precise” for the protected class.\n",
        "\n",
        "Differences between the groups start to emerge if we consider indviduals that are labeled as being likely to re-offend, i.e., the total fraction of positives between the two groups:"
      ],
      "metadata": {
        "id": "2OuRMyVaHrrV"
      }
    }
  ]
}