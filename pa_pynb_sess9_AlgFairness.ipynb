{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj+egAL67sUazWruUDv7A9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess9_AlgFairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 9: Algorithmic Bias and Fairness\n",
        "\n",
        "Jim Guszcza and Dani Bauer, 10/2023"
      ],
      "metadata": {
        "id": "Ccw0SuBI5LYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we discuss approaches how to analyze a predictive algorithm with regards to \"fairness.\" We do so in the context of a well-known case study on an algorithm that assists judges in parole decisions. We go over different notions of fairness, discuss tradeoffs, and explain the intuition behind the results of the analyses. We also discuss approaches how to adjust algorithms to enforce fairness. A second case study, which we will ask you to work on, illustrates how these ideas apply in the actuarial context.\n",
        "\n",
        "### Load required packages"
      ],
      "metadata": {
        "id": "zpUgxOcS5NiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kSv8hx144fah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas\n",
        "#Another library that seems to be popular is fariness 360:\n",
        "#!pip install aif360"
      ],
      "metadata": {
        "id": "BrPUeW1L6Xa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aequitas.group import Group\n",
        "from aequitas.bias import Bias\n",
        "from aequitas.fairness import Fairness\n",
        "import aequitas.plot as ap"
      ],
      "metadata": {
        "id": "EsxbkyTO6by5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compas Case Study\n",
        "\n",
        "The Compas data originates from a [well-known case study on algorithmic bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). The background is [as follows](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm): Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant's likelihood of becoming a recidivist---a term used to describe criminals who re-offend. One of the commercial tools made by Northpointe, Inc. is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). The case study compares outcomes and risk scores for individuals belonging to different races. In what follows, we will go through some of these analyses ourselves.\n",
        "\n",
        "## The Compas Data\n",
        "\n",
        "The data is in our github folder. Let's take a look:"
      ],
      "metadata": {
        "id": "YB2Fhuab_xpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "metadata": {
        "id": "XjM0xW_pATMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv('CAS_PredMod/pa_data_compasdata.csv')\n",
        "dat.head()"
      ],
      "metadata": {
        "id": "2HhtisuQAi3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat.describe()"
      ],
      "metadata": {
        "id": "sR0ZFQweBLV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains information on recidivism of 6,172 individuals as well as information on the individual's age, sex, criminal history, their ethnicity---and the risk score they received from the COMPAS algorithm.\n",
        "\n",
        "To simplify the situation, we focus on two ethnicity levels only: African-Americans and Caucasians."
      ],
      "metadata": {
        "id": "4hYinYNND79l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = dat.loc[(dat['ethnicity'] == 'Caucasian') | (dat['ethnicity'] == 'African_American')]\n",
        "dat.describe()"
      ],
      "metadata": {
        "id": "iE06ij2sD-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we still have the majority of individuals included. We will consider the African-Americans as the \"protected\" class.\n",
        "\n",
        "We commence by exploring the data some and looking at fairness manually, but then we will also explore how to use the Aequitas package in this setting."
      ],
      "metadata": {
        "id": "M696IiNCEyOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness scores vs. recidivism rates\n",
        "\n",
        "Let's start by comparing the COMPAS scores between the two ethic groups (see also the density plots from the fairness package above):"
      ],
      "metadata": {
        "id": "sM_2m9OIGRM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat.loc[dat['ethnicity'] == 'Caucasian']['probability']"
      ],
      "metadata": {
        "id": "xMla0FgbIQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Score distribution by group\")\n",
        "plt.xlabel(\"Group: 1 = Caucasian, 2 = African-American\")\n",
        "plt.boxplot([dat.loc[dat['ethnicity'] == 'Caucasian']['probability'],dat.loc[dat['ethnicity'] == 'African_American']['probability']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WH65Ol1QE2X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of scores in the African-American group has a higher median and higher percentiles than the scores in the Caucasian group.\n",
        "\n",
        "However, consider the number of re-offenders between the group, we see the following:"
      ],
      "metadata": {
        "id": "1cW5E765ON6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "by_race = sns.countplot(x=\"ethnicity\", hue=\"Two_yr_Recidivism\", data=dat[dat.ethnicity.isin(['African_American', 'Caucasian'])], palette=aq_palette)"
      ],
      "metadata": {
        "id": "pMMz2A__PzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider raw averages of re-offenders in the two groups -- for caucasians:"
      ],
      "metadata": {
        "id": "-Agc5sMDP3If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "qI7OQvUNOqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for the protected group:"
      ],
      "metadata": {
        "id": "NnnHTO18PRS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "W7denUplOgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can interpret the higher average COMPAS scores as resulting from statistical facts in this particular population.\n",
        "\n",
        "Let's look at this in more detail: We plot the average scores for those that do not re-offend and for those that do re-offend, and we compare it to the percentage of re-offenders based on the decision that was suggested by the algorithm."
      ],
      "metadata": {
        "id": "e7LIq6T2PT3d"
      }
    }
  ]
}