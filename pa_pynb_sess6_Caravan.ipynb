{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/CAS_PredMod/blob/main/pa_pynb_sess6_Caravan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Caravan Insurance Purchase Case Study\n",
        "\n",
        "In this tutorial, we will introduce one of the examples we will use in the coming tutorials: We will attempt to predict whether individuals purchased a Caravan insurance policy.\n",
        "\n",
        "We will apply several of our predictive models from the past weeks (glm and gam) but also check the performance of a purley algorithmic approach: the *k-nearest-neighbor classifier*. \n",
        "\n",
        "## Review of Concepts and Maths\n",
        "\n",
        "As we saw in a previously, non-linear regression models are powerful approaches for depicting non-linear relationships -- with the key caveat that our explanatory variable had a single dimension only.  Once we venture into higher dimensions -- that means multiple, potentially interrelated features -- obtaining a similar fit will require a *very* large number of data points becaus of the **curse of dimensionality**.  Hence, statistical learning methods (need to) impose structure, in one way or another, and picking a learner in some way depends on picking an appropiate structure. **Generalized Additive Models** (GAMs) -- leverages the performance of non-linear regression models in lower dimensions but imposes an *additive* structure between the functions of the individual features:\n",
        "$$\n",
        "g\\left(\\mathbb{E}\\left[Y | X_1,...,X_p\\right]\\right) = \\alpha + f_1(X_1) + \\ldots + f_p(X_p).\n",
        "$$\n",
        "As such, GAMs take on an intermediate position between linear regression and a general non-linear model:  They generalize the impact each feature can have on the outcome, but they keep the same structure on their relationship.  In particular, they do not allow for rich interactions between the variables -- which is the key downside of GAMs.\n",
        "\n",
        "Other so-called *algorithmic* learners use different structural assumptions. For instance, we illustrate a **k-nearest neighbor (knn)** approach, where the predicted class at a point $x_0$ is chosen based on the $k$ points that are closest:\n",
        "$$\n",
        "y(x_0) = \\max_j\\left\\{\\frac{1}{K} \\sum_{i \\in N_K(x_0)} 1_{\\{y_i=j\\}}\\right\\},\n",
        "$$\n",
        "where $N_k(x_0)$ denotes the index set of the $K$ points in the training sample that are closest to the point $x_0$ (usually in the sense of Euclidean distance).  This is very differnt than what we have seen before in that we don't have an underlying \"probabilistic\" approach.\n",
        "\n",
        "# Caravan Insurance Application\n",
        "\n",
        "We look at the `Caravan` insurance data set included in the ISLR textbook. As indicated in Section 4.6.6, it is a dataset that includes 85 predictors that measure demographic characteristics for 5,822 individuals and \"Purchase,\" which indicates whether or not a given individual purchases a caravan insurance policy. \n",
        "\n",
        "As usual, let's load some relevant libraries:"
      ],
      "metadata": {
        "id": "dHbED4M_i3Vp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKmQZZSUufH"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load our dataset:"
      ],
      "metadata": {
        "id": "zW1Vsrn9jxxz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha1vAQH4VErB"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/CAS_PredMod.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtWs9QsTUufJ"
      },
      "source": [
        "Caravan = pd.read_csv('CAS_PredMod/pa_data_caravan.csv', index_col=0) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Data Exploration"
      ],
      "metadata": {
        "id": "22qXzCaej4ra"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yun-91lhUufL"
      },
      "source": [
        "Caravan.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables 1-43 represent sociodemographic data, variables 44-86 describe product ownership, and Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy.\n",
        "\n",
        "Let's consider some aggregate statistics:"
      ],
      "metadata": {
        "id": "0bJ5JIlqkCC4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BySMAV24UufL"
      },
      "source": [
        "Caravan.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And check how many people purchase insurance:"
      ],
      "metadata": {
        "id": "-6_uDvoYkdtV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6ha6XDUufM"
      },
      "source": [
        "Caravan['Purchase'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So only roughly 6% of all people buy caravan insurance.  That will be costly for an insurance agent because for every client she or he visits, only 6 in 100 will purchase insurance.  So let's use our knowledge about classification to help out the sales force, and let's try to determine individuals (based on their characteristics) that are more likely to purchase a policy.\n",
        "\n",
        "## Predictive Modeling\n",
        "\n",
        "Let's split into a training and test set to get going"
      ],
      "metadata": {
        "id": "ndkNHfnAkhqQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_eYPqIeUufO"
      },
      "source": [
        "Train, Test = train_test_split(Caravan, test_size=0.25, random_state=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRc-UTJwUufR"
      },
      "source": [
        "X_train = Train.drop(['Purchase'], axis=1)\n",
        "y_train = Train['Purchase']\n",
        "X_test = Test.drop(['Purchase'], axis=1)\n",
        "y_test = Test['Purchase']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a vanilla logistic regression model:"
      ],
      "metadata": {
        "id": "E1WDfjihk5vJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLxhzONLUufS"
      },
      "source": [
        "logistic_model = LogisticRegression(fit_intercept=True, max_iter=1000)\n",
        "logistic_model.fit(X_train,y_train)\n",
        "y_pred_logistic = logistic_model.predict(X_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the confusion matrix resulting from our predictions (here the predicted probabilities are already coerced to classes):"
      ],
      "metadata": {
        "id": "Sga9f0aslKB9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fiMB1kDXqNS"
      },
      "source": [
        "confusion_matrix(y_test, y_pred_logistic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't get a single positive one right -- so not great performance.  Of course, we could choose a different cutoff.  Let's evaluate the AUC, where we first have to convert the predictions to probabilities:"
      ],
      "metadata": {
        "id": "SwgMerVCbPUn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pox4agQZUufT"
      },
      "source": [
        "y_pred_logistic = logistic_model.predict_proba(X_test)\n",
        "def Extract(lst): \n",
        "    return [item[0] for item in lst] \n",
        "y_pred_logistic = Extract(y_pred_logistic)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqrGZiD4UufU"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve((Test['Purchase'] == 'No'), y_pred_logistic) \n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic') \n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc) \n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--') \n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1]) \n",
        "plt.ylabel('True Positive Rate') \n",
        "plt.xlabel('False Positive Rate') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare these results to an L1-regularized logistic regression -- a.k.a. LASSO logistic regression -- to see if that yields an improvement.  After all, there are many features so possibily selection is important:"
      ],
      "metadata": {
        "id": "yPQS5Ph-bgHQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpGV3GTNUufS"
      },
      "source": [
        "lassolog_model = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='saga',\n",
        "    max_iter=2000)  # or 'liblinear'\n",
        "lassolog_model.fit(X_train, y_train)\n",
        "y_pred_lassolog = lassolog_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the predictions:"
      ],
      "metadata": {
        "id": "4VybTP5kbov2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-kcNAWaYIP"
      },
      "source": [
        "confusion_matrix(y_test, y_pred_lassolog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to tune the model a bit better (this part takes a bit, so you can skip):"
      ],
      "metadata": {
        "id": "w_z3g8vObrP4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umvGcMO-UufU"
      },
      "source": [
        "C = [50, 10, 1, .1, 0.05,.01,.001]\n",
        "for c in C:\n",
        "  lassologcv_model = LogisticRegression(penalty='l1',C=c,class_weight = 'balanced',solver='liblinear',max_iter=2000)\n",
        "  scores = cross_val_score(lassologcv_model, X_train, y_train, cv=5, scoring=\"f1_micro\")  \n",
        "  print(scores)\n",
        "  print(np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate predictions:"
      ],
      "metadata": {
        "id": "yRb6mumTcSiT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4uZ-k0_bF7d"
      },
      "source": [
        "lassologcv_model = LogisticRegression(penalty='l1',C=50,class_weight = 'balanced',solver='liblinear',max_iter=2000)\n",
        "lassologcv_model.fit(X_train, y_train)\n",
        "y_pred_lassologcv = lassologcv_model.predict(X_test)\n",
        "confusion_matrix(y_test, y_pred_lassologcv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's check the AUC again:"
      ],
      "metadata": {
        "id": "t8K_OmYIcX_0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGUfjYVWUufV"
      },
      "source": [
        "y_pred_lassologcv = lassologcv_model.predict_proba(X_test)\n",
        "y_pred_lassologcv = Extract(y_pred_lassologcv)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8JbflZvUufW"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve((Test['Purchase'] == 'No'), y_pred_lassologcv) \n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic') \n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc) \n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--') \n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1]) \n",
        "plt.ylabel('True Positive Rate') \n",
        "plt.xlabel('False Positive Rate') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No improvement."
      ],
      "metadata": {
        "id": "d5FoaO1Echr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's check the k-nearest neighbor approach:"
      ],
      "metadata": {
        "id": "eueckpXJc7v1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-puXKZlgUufW"
      },
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "confusion_matrix(y_test, y_pred_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So at least we're getting one :-)"
      ],
      "metadata": {
        "id": "VyfoR0Mohx1X"
      }
    }
  ]
}